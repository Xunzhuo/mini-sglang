---
sidebar_position: 2
---

# 语言模型简史：从 N-gram 到 GPT

语言模型 (Language Model, LM) 是自然语言处理的核心技术。本文将带你回顾语言模型的发展历程，理解为什么 Transformer 架构能够取得突破性成功。

## 什么是语言模型？

语言模型的本质是**预测下一个词的概率分布**：

```
P(w_t | w_1, w_2, ..., w_{t-1})
```

给定前面的词序列，模型预测下一个词最可能是什么。这个看似简单的任务，实际上需要模型理解语法、语义，甚至世界知识。

## 第一阶段：统计语言模型

### N-gram 模型 (1980s-2000s)

N-gram 是最早的语言模型方法，基于**马尔可夫假设**：当前词只依赖于前 N-1 个词。

```
Bigram (N=2):  P(w_t | w_{t-1})
Trigram (N=3): P(w_t | w_{t-2}, w_{t-1})
```

**示例**：预测 "我爱吃___"
- 统计语料库中 "我爱吃" 后面出现的词频
- "苹果" 出现 100 次，"西瓜" 出现 50 次
- P(苹果|我爱吃) = 100/150 = 0.67

**局限性**：
- 无法捕捉长距离依赖
- 数据稀疏问题（很多 N-gram 组合从未出现）
- 需要大量存储空间

## 第二阶段：神经网络语言模型

### 词向量的诞生 (2003)

Bengio 等人提出**神经概率语言模型**，引入词向量 (Word Embedding) 概念：

- 将离散的词映射到连续的向量空间
- 语义相似的词，向量也相近
- 解决了数据稀疏问题

### Word2Vec (2013)

Mikolov 提出 Word2Vec，让词向量训练变得高效：

- **CBOW**：根据上下文预测中心词
- **Skip-gram**：根据中心词预测上下文

著名的词向量算术：`King - Man + Woman ≈ Queen`

## 第三阶段：循环神经网络

### RNN (1986)

循环神经网络通过**隐藏状态**传递历史信息：

```
h_t = f(h_{t-1}, x_t)
y_t = g(h_t)
```

每个时间步的隐藏状态 h_t 包含了之前所有输入的信息。

**问题**：梯度消失/爆炸，难以学习长距离依赖。

### LSTM (1997)

长短期记忆网络引入**门控机制**：

- **遗忘门**：决定丢弃哪些信息
- **输入门**：决定保存哪些新信息
- **输出门**：决定输出哪些信息

LSTM 成功缓解了梯度消失问题，成为 2010s 的主流架构。

### GRU (2014)

门控循环单元是 LSTM 的简化版本，参数更少，效果相当。

## 第四阶段：Seq2Seq 与注意力机制

### Seq2Seq (2014)

序列到序列模型为机器翻译带来突破：

```
Encoder: 输入序列 → 上下文向量
Decoder: 上下文向量 → 输出序列
```

**问题**：整个输入序列压缩到一个固定长度向量，信息瓶颈严重。

### 注意力机制 (Attention, 2015)

Bahdanau 等人提出注意力机制，允许 Decoder 在每一步"关注"输入序列的不同位置：

```
context_t = Σ α_ti · h_i
```

其中 α_ti 是注意力权重，表示生成第 t 个输出时，对输入第 i 个位置的关注程度。

**核心思想**：不再压缩成固定向量，而是动态选择相关信息。

## 第五阶段：Transformer 革命 (2017)

### "Attention is All You Need"

Google 提出 Transformer 架构，完全抛弃循环结构：

**核心创新**：
- **自注意力 (Self-Attention)**：序列内部的元素相互关注
- **多头注意力 (Multi-Head Attention)**：从多个角度学习关系
- **位置编码 (Positional Encoding)**：注入位置信息
- **并行计算**：摆脱 RNN 的顺序依赖

### 为什么 Transformer 成功？

| 对比维度 | RNN/LSTM | Transformer |
|----------|----------|-------------|
| 并行能力 | 差（顺序处理） | 强（完全并行） |
| 长距离依赖 | 信息衰减 | 直接连接 |
| 训练速度 | 慢 | 快 |
| 可扩展性 | 有限 | 极强 |

## 第六阶段：预训练大模型时代

### GPT 系列 (OpenAI)

- **GPT-1 (2018)**：证明预训练+微调范式有效
- **GPT-2 (2019)**：展示零样本学习能力
- **GPT-3 (2020)**：175B 参数，涌现出上下文学习能力
- **GPT-4 (2023)**：多模态，性能大幅提升

### BERT (Google, 2018)

双向编码器，擅长理解任务：
- 完形填空式预训练 (MLM)
- 句子关系预测 (NSP)

### 技术路线分化

| 路线 | 代表模型 | 特点 | 适用场景 |
|------|----------|------|----------|
| **Decoder-only** | GPT 系列 | 自回归生成 | 文本生成、对话 |
| **Encoder-only** | BERT | 双向理解 | 分类、NER |
| **Encoder-Decoder** | T5, BART | 编码+生成 | 翻译、摘要 |

## 语言模型的涌现能力

当模型规模足够大时，会"涌现"出意想不到的能力：

- **上下文学习 (In-Context Learning)**：无需微调，通过示例学习
- **思维链 (Chain-of-Thought)**：逐步推理复杂问题
- **指令遵循 (Instruction Following)**：理解并执行复杂指令

## 本章小结

| 阶段 | 时间 | 代表技术 | 突破点 |
|------|------|----------|--------|
| 统计方法 | 1980s-2000s | N-gram | 概率建模 |
| 神经网络 | 2003-2013 | Word2Vec | 词向量 |
| 循环网络 | 2014-2016 | LSTM | 序列建模 |
| 注意力 | 2015-2017 | Attention | 动态聚焦 |
| Transformer | 2017-至今 | GPT, BERT | 并行+扩展 |

## 延伸阅读

- Attention Is All You Need (2017)
- Language Models are Few-Shot Learners (GPT-3, 2020)
- BERT: Pre-training of Deep Bidirectional Transformers (2018)

---

*下一篇：[分词器：文本到数字的桥梁](./03-tokenization.md)*
