---
sidebar_position: 11.5
---

# RL 策略：PPO、GRPO、DPO

在 LLM 训练中，强化学习（RL）策略用于让模型的输出与人类偏好对齐。本文将深入介绍三种主流的 RL 策略：PPO、GRPO 和 DPO，帮助你理解它们的原理和适用场景。

## 1. 为什么需要 RL？

SFT 让模型学会了遵循指令，但仍存在问题：

```mermaid
graph LR
    subgraph SFT的局限
        S1[回答正确但不友好]
        S2[拒绝回答安全问题]
        S3[冗长啰嗦]
        S4[过度自信]
    end
    
    SFT模型 --> S1 & S2 & S3 & S4
```

**RL 的目标**：让模型输出更符合人类偏好

- 更有帮助 (Helpful)
- 更安全 (Harmless)
- 更诚实 (Honest)

## 2. RL 策略发展历程

```mermaid
timeline
    title LLM RL策略演进
    section RLHF时代
        2020 : InstructGPT
             : PPO + Reward Model
        2022 : ChatGPT
             : PPO大规模应用
    section 简化时代
        2023 : DPO
             : 无需Reward Model
        2024 : GRPO
             : 更简单的策略优化
        2024 : IPO, KTO
             : 更多变体
```

## 3. PPO (Proximal Policy Optimization)

PPO 是最经典的 RLHF 算法，被 OpenAI 用于训练 ChatGPT。

### 3.1 PPO 训练流程

```mermaid
graph TB
    subgraph PPO训练循环
        SFT[SFT模型] --> RM[训练Reward Model]
        RM --> P[策略模型生成回答]
        P --> R[Reward Model打分]
        R --> U[PPO更新策略]
        U --> |迭代| P
    end
```

**四个模型**：
1. **Policy Model (Actor)**：当前要训练的模型
2. **Reference Model**：冻结的 SFT 模型，防止偏离太远
3. **Reward Model**：评估回答质量
4. **Value Model (Critic)**：估计状态价值

### 3.2 PPO 目标函数

PPO 的核心是**限制策略更新幅度**，防止训练不稳定。

**核心思想**：计算新旧策略的概率比 r(θ)，然后用一个裁剪函数限制这个比值在 [1-ε, 1+ε] 范围内（通常 ε=0.2）。这样即使优势函数 A 很大，策略更新也不会过于激进。

```mermaid
graph LR
    subgraph PPO裁剪
        R["r(θ) = 新策略/旧策略"]
        R --> C["clip(r, 0.8, 1.2)"]
        C --> L["防止更新过大"]
    end
```

### 3.3 PPO 训练要点

PPO 训练需要设置多个关键参数：
- **学习率**：通常为 1e-5，比 SFT 更小
- **批次大小**：较大的批次（如 128）有助于稳定训练
- **KL 惩罚**：控制新策略与参考策略的差距，目标 KL 通常设为 0.1
- **PPO epochs**：每批数据更新的次数，通常 4 次

### 3.4 PPO 的优缺点

| 优点 | 缺点 |
|------|------|
| 经过大规模验证 | 训练复杂（4个模型） |
| 效果稳定 | 显存需求大 |
| 细粒度控制 | 超参数敏感 |
| | 需要训练 Reward Model |

## 4. DPO (Direct Preference Optimization)

DPO 是 2023 年提出的革命性方法，**直接从偏好数据学习，无需 Reward Model**。

### 4.1 DPO 的核心思想

```mermaid
graph LR
    subgraph RLHF/PPO
        D1[偏好数据] --> RM[训练Reward Model]
        RM --> RL[RL训练]
        RL --> M1[对齐模型]
    end
    
    subgraph DPO
        D2[偏好数据] --> DL[直接优化]
        DL --> M2[对齐模型]
    end
```

**关键洞察**：Reward Model 可以被解析地求解，直接用偏好数据训练！

### 4.2 DPO 数学原理

DPO 从 RLHF 的目标函数出发，证明了最优策略可以用一个闭式解表示。由此推导出 DPO 损失函数：给定一个 prompt 和两个回答（chosen 是人类偏好的回答，rejected 是不偏好的回答），DPO 优化模型让 chosen 的对数概率相对于参考模型的提升大于 rejected 的提升。

**关键参数**：
- **β（温度）**：控制 KL 惩罚强度，通常设为 0.1
- 较大的 β 使模型更保守，较小的 β 使模型更激进

### 4.3 DPO 数据格式

DPO 需要偏好数据对，每条数据包含：
- **prompt**：用户输入
- **chosen**：人类偏好的（更好的）回答
- **rejected**：人类不偏好的（较差的）回答

```mermaid
graph LR
    subgraph DPO数据
        P[Prompt] --> C[Chosen 好回答]
        P --> R[Rejected 差回答]
    end
```

### 4.4 DPO 训练要点

DPO 训练相对简单：
- **只需要两个模型**：当前模型和冻结的参考模型
- **学习率更小**：通常 5e-7，比 SFT 小 10-100 倍
- **训练轮数少**：1-3 个 epoch，防止过拟合
- **数据质量关键**：chosen 和 rejected 差异要明显

### 4.5 DPO 的优缺点

| 优点 | 缺点 |
|------|------|
| 无需 Reward Model | 需要偏好数据对 |
| 训练简单（2个模型） | 无法在线优化 |
| 显存需求小 | 对数据质量敏感 |
| 稳定易调 | 可能欠拟合复杂偏好 |

## 5. GRPO (Group Relative Policy Optimization)

GRPO 是 DeepSeek 在 2024 年提出的方法，进一步简化了 RL 训练。

### 5.1 GRPO 核心思想

```mermaid
graph TB
    subgraph GRPO思想
        P[Prompt] --> G[生成多个回答]
        G --> R[Reward Model打分]
        R --> C[组内相对排序]
        C --> O[优化相对优势]
    end
```

**关键创新**：
- **不需要 Critic/Value Model**
- 使用**组内相对奖励**替代绝对奖励
- 更简单的优化目标

### 5.2 GRPO vs PPO

| 特性 | PPO | GRPO |
|------|-----|------|
| **模型数量** | 4个 | 2个 |
| **Value Model** | 需要 | 不需要 |
| **奖励计算** | 绝对奖励 | 组内相对奖励 |
| **训练复杂度** | 高 | 低 |
| **显存需求** | 大 | 小 |

### 5.3 GRPO 算法流程

GRPO 的训练流程如下：
1. **生成一组回答**：对每个 prompt 生成 G 个不同的回答
2. **计算奖励**：用 Reward Model 对每个回答打分
3. **计算组内相对优势**：每个回答的优势 = 该回答奖励 - 组内平均奖励
4. **策略更新**：使用相对优势来更新策略，优势为正的回答被鼓励，优势为负的被抑制

### 5.4 GRPO 的优势

```mermaid
graph LR
    subgraph GRPO优势
        S1[训练更简单]
        S2[显存更少]
        S3[效果相当]
    end
    
    S1 & S2 & S3 --> R[更实用的RL方案]
```

## 6. 其他 RL 变体

### 6.1 IPO (Identity Preference Optimization)

针对 DPO 可能过拟合的问题，IPO 添加了正则项，使优化更加稳定。

### 6.2 KTO (Kahneman-Tversky Optimization)

不需要配对数据，只需要单独的好/坏标注：

```mermaid
graph LR
    subgraph KTO数据
        P1[Prompt + 好回答 + 标签:1]
        P2[Prompt + 坏回答 + 标签:0]
    end
```

### 6.3 ORPO (Odds Ratio Preference Optimization)

将 SFT 和偏好优化合并为一步，同时优化语言建模损失和偏好优化损失。

## 7. 方法对比

```mermaid
graph TB
    subgraph RL方法比较
        PPO[PPO<br/>最成熟<br/>最复杂]
        DPO[DPO<br/>最简单<br/>无需RM]
        GRPO[GRPO<br/>折中方案<br/>无需Value]
    end
    
    PPO --> |简化| DPO
    PPO --> |简化| GRPO
```

| 方法 | Reward Model | Value Model | 偏好数据 | 训练复杂度 | 效果 |
|------|-------------|-------------|----------|-----------|------|
| **PPO** | 需要 | 需要 | 不直接需要 | 高 | 最好 |
| **DPO** | 不需要 | 不需要 | 需要配对 | 低 | 很好 |
| **GRPO** | 需要 | 不需要 | 不直接需要 | 中 | 很好 |
| **KTO** | 不需要 | 不需要 | 单独标注 | 低 | 好 |
| **ORPO** | 不需要 | 不需要 | 需要配对 | 低 | 好 |

## 8. 如何选择？

```mermaid
graph TB
    Start[选择RL策略] --> Q1{有偏好数据对?}
    
    Q1 --> |是| Q2{需要最佳效果?}
    Q1 --> |否| Q3{有好/坏标注?}
    
    Q2 --> |是| PPO[使用PPO]
    Q2 --> |否| DPO[使用DPO]
    
    Q3 --> |是| KTO[使用KTO]
    Q3 --> |否| GRPO[使用GRPO+RM]
```

**推荐选择**：

| 场景 | 推荐方法 | 原因 |
|------|----------|------|
| **资源充足，追求最佳** | PPO | 效果最好，经过验证 |
| **有偏好数据，快速迭代** | DPO | 简单高效 |
| **资源有限，需要 RM** | GRPO | 比 PPO 简单 |
| **只有好/坏标注** | KTO | 数据要求最低 |

## 9. 实战建议

### 9.1 数据准备

好的偏好数据应该具有以下特征：
- chosen 和 rejected 差异明显
- 覆盖多种场景和任务类型
- 标注一致性高

### 9.2 超参数调优

**DPO 常用超参数**：
- β（温度）：0.05-0.5，越大越保守
- 学习率：5e-7，比 SFT 小 10-100 倍
- 训练轮数：1-3 个 epoch，不宜过多防止过拟合

**PPO 常用超参数**：
- KL 惩罚系数：0.1
- PPO 裁剪范围：0.2
- 学习率：1e-5

## 10. 本章小结

```mermaid
mindmap
  root((RL策略))
    PPO
      最成熟
      4个模型
      效果最好
    DPO
      最简单
      无需RM
      偏好数据
    GRPO
      折中方案
      无需Value
      组内相对
    其他
      IPO
      KTO
      ORPO
```

**核心要点**：
- PPO 是最成熟的方案，但复杂度高
- DPO 无需 Reward Model，大幅简化训练
- GRPO 在 PPO 和 DPO 之间取得平衡
- 选择方法时考虑数据、资源和效果需求

## 延伸阅读

- [Proximal Policy Optimization Algorithms (PPO)](https://arxiv.org/abs/1707.06347)
- [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290)
- [DeepSeekMath: GRPO](https://arxiv.org/abs/2402.03300)
- [KTO: Model Alignment as Prospect Theoretic Optimization](https://arxiv.org/abs/2402.01306)

---

*下一篇：[RLHF：对齐人类偏好](./12-rlhf.md)*
