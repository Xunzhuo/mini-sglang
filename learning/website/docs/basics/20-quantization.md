---
sidebar_position: 20
---

# æ¨¡å‹é‡åŒ–ï¼šç”¨æ›´å°‘èµ„æºè¿è¡Œå¤§æ¨¡å‹

æ¨¡å‹é‡åŒ– (Quantization) æ˜¯å°†æ¨¡å‹å‚æ•°ä»é«˜ç²¾åº¦ï¼ˆå¦‚ FP16ï¼‰è½¬æ¢ä¸ºä½ç²¾åº¦ï¼ˆå¦‚ INT8/INT4ï¼‰çš„æŠ€æœ¯ï¼Œå¯ä»¥æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨å’ŒåŠ é€Ÿæ¨ç†ã€‚

## ä¸ºä»€ä¹ˆéœ€è¦é‡åŒ–ï¼Ÿ

### æ˜¾å­˜å ç”¨å¯¹æ¯”

```
7B æ¨¡å‹:
- FP32: 7B Ã— 4 bytes = 28 GB
- FP16: 7B Ã— 2 bytes = 14 GB
- INT8: 7B Ã— 1 byte  = 7 GB
- INT4: 7B Ã— 0.5 byte = 3.5 GB

é‡åŒ–è®©æ¶ˆè´¹çº§ GPU (24GB) ä¹Ÿèƒ½è¿è¡Œ 70B æ¨¡å‹ï¼
```

### é‡åŒ–çš„å¥½å¤„

- ğŸ“‰ **æ˜¾å­˜å ç”¨é™ä½**ï¼šINT4 åªéœ€ FP16 çš„ 1/4
- âš¡ **æ¨ç†é€Ÿåº¦æå‡**ï¼šå†…å­˜å¸¦å®½æ˜¯æ¨ç†ç“¶é¢ˆ
- ğŸ’° **éƒ¨ç½²æˆæœ¬é™ä½**ï¼šå¯ç”¨æ›´ä¾¿å®œçš„ç¡¬ä»¶

## é‡åŒ–åŸºç¡€

### æ•°æ®ç±»å‹å›é¡¾

| ç±»å‹ | ä½å®½ | èŒƒå›´ | ç²¾åº¦ |
|------|------|------|------|
| FP32 | 32 | Â±3.4Ã—10Â³â¸ | é«˜ |
| FP16 | 16 | Â±65504 | ä¸­ |
| BF16 | 16 | Â±3.4Ã—10Â³â¸ | ä½ |
| INT8 | 8 | -128~127 | æ•´æ•° |
| INT4 | 4 | -8~7 | æ•´æ•° |

### é‡åŒ–å…¬å¼

å°†æµ®ç‚¹æ•°æ˜ å°„åˆ°æ•´æ•°ï¼š

```
é‡åŒ–: q = round(x / scale) + zero_point
åé‡åŒ–: x' = (q - zero_point) Ã— scale
```

### é‡åŒ–ç²’åº¦

| ç²’åº¦ | æè¿° | ç²¾åº¦ | å¼€é”€ |
|------|------|------|------|
| Per-tensor | æ•´ä¸ªå¼ é‡å…±äº« scale | ä½ | ä½ |
| Per-channel | æ¯ä¸ªé€šé“ä¸€ä¸ª scale | ä¸­ | ä¸­ |
| Per-group | æ¯ N ä¸ªå…ƒç´ ä¸€ä¸ª scale | é«˜ | é«˜ |

## è®­ç»ƒåé‡åŒ– (PTQ)

### åŸºæœ¬æ–¹æ³•

ç›´æ¥åœ¨è®­ç»ƒå¥½çš„æ¨¡å‹ä¸Šè¿›è¡Œé‡åŒ–ï¼Œæ— éœ€é‡æ–°è®­ç»ƒï¼š

```python
import torch

def naive_quantize(tensor, n_bits=8):
    # è®¡ç®— scale å’Œ zero_point
    min_val, max_val = tensor.min(), tensor.max()
    scale = (max_val - min_val) / (2**n_bits - 1)
    zero_point = round(-min_val / scale)
    
    # é‡åŒ–
    q_tensor = torch.round(tensor / scale + zero_point)
    q_tensor = torch.clamp(q_tensor, 0, 2**n_bits - 1)
    
    return q_tensor.to(torch.int8), scale, zero_point
```

### æ ¡å‡† (Calibration)

ä½¿ç”¨å°‘é‡æ•°æ®ç¡®å®šæœ€ä½³é‡åŒ–å‚æ•°ï¼š

```python
def calibrate(model, calibration_data):
    # æ”¶é›†æ¯å±‚æ¿€æ´»å€¼çš„ç»Ÿè®¡ä¿¡æ¯
    for batch in calibration_data:
        model(batch)
        # è®°å½• min/max æˆ–ç›´æ–¹å›¾
    
    # ç¡®å®šæœ€ä½³ scale å’Œ zero_point
    return quantization_params
```

## LLM.int8()

### å¼‚å¸¸å€¼é—®é¢˜

LLM ä¸­å­˜åœ¨å°‘é‡**å¼‚å¸¸å€¼ (Outliers)**ï¼Œç›´æ¥é‡åŒ–ä¼šå¯¼è‡´ç²¾åº¦æŸå¤±ï¼š

```
å¤§éƒ¨åˆ†æƒé‡: [-0.5, 0.5]
å¼‚å¸¸å€¼:     [-10, 10] æˆ–æ›´å¤§

å¦‚æœç”¨ç»Ÿä¸€çš„ scaleï¼Œæ­£å¸¸å€¼ç²¾åº¦æŸå¤±ä¸¥é‡
```

### æ··åˆç²¾åº¦æ–¹æ¡ˆ

```
1. æ£€æµ‹å¼‚å¸¸å€¼ï¼ˆç»å¯¹å€¼ > é˜ˆå€¼çš„ç»´åº¦ï¼‰
2. å¼‚å¸¸ç»´åº¦ä¿æŒ FP16
3. å…¶ä½™ç»´åº¦ä½¿ç”¨ INT8
4. åˆ†åˆ«è®¡ç®—ååˆå¹¶
```

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    load_in_8bit=True,  # LLM.int8()
    device_map="auto",
)
```

## GPTQ

### æ ¸å¿ƒæ€æƒ³

é€å±‚é‡åŒ–ï¼ŒåŒæ—¶æœ€å°åŒ–é‡åŒ–è¯¯å·®ï¼š

```
ç›®æ ‡: min ||WX - Q(W)X||Â²

æ¯æ¬¡é‡åŒ–ä¸€ä¸ªæƒé‡ï¼Œè°ƒæ•´å‰©ä½™æƒé‡æ¥è¡¥å¿è¯¯å·®
```

### Optimal Brain Quantization (OBQ)

åŸºäº Hessian çŸ©é˜µçš„æœ€ä¼˜é‡åŒ–é¡ºåºï¼š

```python
# ä¼ªä»£ç 
for i in range(n_weights):
    # é€‰æ‹©é‡åŒ–è¯¯å·®æœ€å°çš„æƒé‡
    idx = argmin(quant_error)
    
    # é‡åŒ–è¯¥æƒé‡
    W[idx] = quantize(W[idx])
    
    # è°ƒæ•´å‰©ä½™æƒé‡è¡¥å¿è¯¯å·®
    W[remaining] -= H_inv[remaining, idx] * error[idx]
```

### ä½¿ç”¨ GPTQ

```python
from transformers import AutoModelForCausalLM, GPTQConfig

quantization_config = GPTQConfig(
    bits=4,
    dataset="c4",  # æ ¡å‡†æ•°æ®é›†
    group_size=128,
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=quantization_config,
    device_map="auto",
)
```

## AWQ (Activation-aware Weight Quantization)

### æ ¸å¿ƒè§‚å¯Ÿ

ä¸åŒæƒé‡çš„é‡è¦æ€§ä¸åŒã€‚**æ¿€æ´»å€¼å¤§**çš„å¯¹åº”æƒé‡æ›´é‡è¦ã€‚

### æ–¹æ³•

```
1. åˆ†ææ¿€æ´»å€¼åˆ†å¸ƒ
2. è¯†åˆ«é‡è¦æƒé‡ï¼ˆå¯¹åº”å¤§æ¿€æ´»å€¼çš„åˆ—ï¼‰
3. å¯¹é‡è¦æƒé‡ç¼©æ”¾åå†é‡åŒ–
4. æ¨ç†æ—¶åå‘ç¼©æ”¾
```

```python
# æƒé‡é‡è¦æ€§ âˆ å¯¹åº”æ¿€æ´»å€¼çš„å‡å€¼
importance = activation.abs().mean(dim=0)

# ç¼©æ”¾å› å­
scale = (importance / importance.max()) ** alpha

# ç¼©æ”¾åé‡åŒ–
W_scaled = W * scale
W_quant = quantize(W_scaled)

# æ¨ç†æ—¶: output = (W_quant / scale) @ activation
```

### ä½¿ç”¨ AWQ

```python
from awq import AutoAWQForCausalLM

model = AutoAWQForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    safetensors=True,
)

# é‡åŒ–
model.quantize(
    tokenizer,
    quant_config={"w_bit": 4, "q_group_size": 128}
)
```

## GGUF/GGML

### ç‰¹ç‚¹

- ä¸“ä¸º CPU æ¨ç†ä¼˜åŒ–
- æ”¯æŒå¤šç§é‡åŒ–æ ¼å¼
- è¢« llama.cpp å¹¿æ³›ä½¿ç”¨

### é‡åŒ–ç±»å‹

| ç±»å‹ | æè¿° | å¤§å° (7B) |
|------|------|-----------|
| Q2_K | 2-bit | ~2.5 GB |
| Q4_0 | 4-bit | ~4 GB |
| Q4_K_M | 4-bit æ··åˆ | ~4.5 GB |
| Q5_K_M | 5-bit æ··åˆ | ~5 GB |
| Q8_0 | 8-bit | ~7 GB |

### ä½¿ç”¨ llama.cpp

```bash
# è½¬æ¢ä¸º GGUF
python convert.py model_path --outtype f16 --outfile model.gguf

# é‡åŒ–
./quantize model.gguf model-q4_k_m.gguf q4_k_m

# æ¨ç†
./main -m model-q4_k_m.gguf -p "Hello, world"
```

## é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT)

### ä¸ PTQ çš„åŒºåˆ«

```
PTQ: è®­ç»ƒå®Œæˆ â†’ é‡åŒ– â†’ éƒ¨ç½²
QAT: è®­ç»ƒæ—¶æ¨¡æ‹Ÿé‡åŒ– â†’ é‡åŒ– â†’ éƒ¨ç½²
```

### ç›´é€šä¼°è®¡å™¨ (STE)

é‡åŒ–æ“ä½œä¸å¯å¾®ï¼Œä½¿ç”¨ STE è¿‘ä¼¼æ¢¯åº¦ï¼š

```python
class QuantizeFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, scale, zero_point):
        # å‰å‘: çœŸå®é‡åŒ–
        return torch.round(x / scale + zero_point) * scale - zero_point * scale
    
    @staticmethod
    def backward(ctx, grad_output):
        # åå‘: ç›´æ¥ä¼ é€’æ¢¯åº¦ (STE)
        return grad_output, None, None
```

### QLoRA

ç»“åˆ LoRA å’Œé‡åŒ–ï¼Œåœ¨é‡åŒ–æ¨¡å‹ä¸Šé«˜æ•ˆå¾®è°ƒï¼š

```python
from peft import prepare_model_for_kbit_training, LoraConfig

# 4-bit é‡åŒ–åŠ è½½
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# å‡†å¤‡ QLoRA è®­ç»ƒ
model = prepare_model_for_kbit_training(model)

# æ·»åŠ  LoRA
lora_config = LoraConfig(r=16, lora_alpha=32, ...)
model = get_peft_model(model, lora_config)
```

## é‡åŒ–æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ç²¾åº¦æŸå¤± | é€Ÿåº¦æå‡ | æ˜¾å­˜èŠ‚çœ | æ˜“ç”¨æ€§ |
|------|----------|----------|----------|--------|
| LLM.int8() | å° | ä¸­ | 50% | é«˜ |
| GPTQ | å° | å¤§ | 75% | ä¸­ |
| AWQ | å¾ˆå° | å¤§ | 75% | ä¸­ |
| GGUF Q4 | ä¸­ | å¤§ | 75% | é«˜ |

## å®æˆ˜ï¼šé‡åŒ–å¹¶éƒ¨ç½²æ¨¡å‹

```python
# æ–¹æ³•1: ä½¿ç”¨ bitsandbytes (ç®€å•)
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-70b-hf",
    quantization_config=bnb_config,
    device_map="auto",
)

# æ–¹æ³•2: ä½¿ç”¨é¢„é‡åŒ–æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "TheBloke/Llama-2-70B-GPTQ",
    device_map="auto",
)
```

## æœ¬ç« å°ç»“

- é‡åŒ–å°†æ¨¡å‹å‚æ•°è½¬ä¸ºä½ç²¾åº¦ï¼Œé™ä½æ˜¾å­˜å’ŒåŠ é€Ÿæ¨ç†
- PTQ ç®€å•å¿«é€Ÿï¼ŒQAT ç²¾åº¦æ›´é«˜
- GPTQã€AWQ æ˜¯ç›®å‰ LLM é‡åŒ–çš„ä¸»æµæ–¹æ³•
- ä¸åŒåœºæ™¯é€‰æ‹©ä¸åŒæ–¹æ³•ï¼š
  - å¿«é€Ÿéƒ¨ç½²ï¼šbitsandbytes
  - æè‡´æ€§èƒ½ï¼šGPTQ/AWQ
  - CPU æ¨ç†ï¼šGGUF

## å»¶ä¼¸é˜…è¯»

- LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale
- GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers
- AWQ: Activation-aware Weight Quantization

---

*æœ¬ç« æ˜¯åŸºç¡€çŸ¥è¯†ç³»åˆ—çš„æœ€åä¸€ç¯‡ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬è¿›å…¥æ¨ç†å®æˆ˜ï¼*
