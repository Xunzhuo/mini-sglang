---
sidebar_position: 11
---

# ç›‘ç£å¾®è°ƒï¼šè®©æ¨¡å‹å­¦ä¼šå¯¹è¯

ç»è¿‡é¢„è®­ç»ƒçš„æ¨¡å‹è™½ç„¶å…·å¤‡å¼ºå¤§çš„è¯­è¨€èƒ½åŠ›ï¼Œä½†å®ƒåªä¼š"ç»­å†™"æ–‡æœ¬ï¼Œæ— æ³•ç†è§£æŒ‡ä»¤æˆ–è¿›è¡Œå¯¹è¯ã€‚ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning, SFT) æ˜¯è®©æ¨¡å‹å­¦ä¼šéµå¾ªæŒ‡ä»¤çš„å…³é”®æ­¥éª¤ã€‚

## 1. ä»åŸºåº§æ¨¡å‹åˆ°å¯¹è¯æ¨¡å‹

```mermaid
graph LR
    Base[é¢„è®­ç»ƒæ¨¡å‹<br/>Base Model] --> SFT[ç›‘ç£å¾®è°ƒ<br/>SFT]
    SFT --> Instruct[æŒ‡ä»¤å¾®è°ƒæ¨¡å‹<br/>Instruct Model]
    Instruct --> RLHF[RLHF/DPO]
    RLHF --> Chat[å¯¹é½æ¨¡å‹<br/>Chat Model]
```

### 1.1 åŸºåº§æ¨¡å‹çš„è¡Œä¸º

åŸºåº§æ¨¡å‹åªå­¦ä¼šäº†"ç»­å†™"ï¼Œä¸ç†è§£æŒ‡ä»¤ï¼š

```
ç”¨æˆ·: è¯·ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åº
åŸºåº§æ¨¡å‹: å‡½æ•°ã€‚å¿«é€Ÿæ’åºæ˜¯ä¸€ç§é«˜æ•ˆçš„æ’åºç®—æ³•ï¼Œç”±Tony Hoareäº1959å¹´æå‡º...
        ï¼ˆåªæ˜¯åœ¨ç»­å†™ï¼Œä¸æ˜¯åœ¨å›ç­”é—®é¢˜ï¼ï¼‰
```

### 1.2 SFT åçš„è¡Œä¸º

SFT åçš„æ¨¡å‹ç†è§£æŒ‡ä»¤ï¼Œç»™å‡ºæœ‰ç”¨çš„å›ç­”ï¼š

```
ç”¨æˆ·: è¯·ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åº
SFTæ¨¡å‹: 
```python
def quicksort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quicksort(left) + middle + quicksort(right)
```
```

## 2. æŒ‡ä»¤æ•°æ®æ ¼å¼

### 2.1 å•è½®å¯¹è¯æ ¼å¼ (Alpaca)

```json
{
  "instruction": "å°†ä»¥ä¸‹å¥å­ç¿»è¯‘æˆè‹±æ–‡",
  "input": "ä»Šå¤©å¤©æ°”çœŸå¥½",
  "output": "The weather is really nice today."
}
```

### 2.2 å¤šè½®å¯¹è¯æ ¼å¼ (ShareGPT)

```json
{
  "conversations": [
    {"from": "human", "value": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"},
    {"from": "gpt", "value": "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ä½ å›ç­”é—®é¢˜ã€å†™ä½œã€ç¼–ç¨‹ç­‰ã€‚"},
    {"from": "human", "value": "ä½ èƒ½å¸®æˆ‘å†™ä»£ç å—ï¼Ÿ"},
    {"from": "gpt", "value": "å½“ç„¶å¯ä»¥ï¼è¯·å‘Šè¯‰æˆ‘ä½ éœ€è¦ä»€ä¹ˆåŠŸèƒ½ï¼Œæˆ‘ä¼šå°½åŠ›å¸®åŠ©ä½ ã€‚"}
  ]
}
```

### 2.3 Chat Template

ä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒçš„å¯¹è¯æ¨¡æ¿ï¼Œè¿™æ˜¯æ¨¡å‹ç†è§£å¯¹è¯ç»“æ„çš„å…³é”®ï¼š

```mermaid
graph TB
    subgraph å¸¸è§Chat Template
        L[LLaMA-2 Chat<br/>INSTæ ‡ç­¾]
        C[ChatML<br/>im_startæ ‡ç­¾]
        V[Vicuna<br/>USER/ASSISTANT]
    end
```

**LLaMA-2 Chat**:
```
<s>[INST] <<SYS>>
You are a helpful assistant.
<</SYS>>

{user_message_1} [/INST] {assistant_response_1} </s><s>[INST] {user_message_2} [/INST]
```

**ChatML (Qwen, OpenAI)**:
```
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
{user_message}<|im_end|>
<|im_start|>assistant
{assistant_response}<|im_end|>
```

**Vicuna**:
```
A chat between a curious user and an AI assistant.

USER: {user_message}
ASSISTANT: {assistant_response}
```

## 3. é«˜è´¨é‡æ•°æ®é›†

### 3.1 å…¬å¼€æ•°æ®é›†

```mermaid
graph TB
    subgraph å¸¸ç”¨SFTæ•°æ®é›†
        A[Alpaca 52K<br/>Stanford] --> D[SFTæ•°æ®]
        B[ShareGPT 90K<br/>çœŸå®å¯¹è¯] --> D
        C[FLAN 1.8M<br/>å¤šä»»åŠ¡] --> D
        E[UltraChat 1.5M<br/>å¤šè½®å¯¹è¯] --> D
    end
```

| æ•°æ®é›† | è§„æ¨¡ | ç‰¹ç‚¹ | æ¥æº |
|--------|------|------|------|
| **Alpaca** | 52K | Self-Instruct ç”Ÿæˆ | Stanford |
| **ShareGPT** | ~90K | çœŸå®ç”¨æˆ·å¯¹è¯ | ç¤¾åŒºæ”¶é›† |
| **OpenAssistant** | 160K | ä¼—åŒ…æ ‡æ³¨ | LAION |
| **FLAN** | 1.8M | å¤šä»»åŠ¡æŒ‡ä»¤ | Google |
| **WizardLM** | 250K | å¤æ‚æŒ‡ä»¤è¿›åŒ– | Microsoft |
| **UltraChat** | 1.5M | å¤šè½®å¯¹è¯ | æ¸…å |

### 3.2 æ•°æ®è´¨é‡ > æ•°æ®æ•°é‡

**LIMA è®ºæ–‡çš„å‘ç°**ï¼šä»…ç”¨ **1000 æ¡**é«˜è´¨é‡æ•°æ®å°±èƒ½è®­ç»ƒå‡ºä¸é”™çš„å¯¹è¯æ¨¡å‹ï¼

```mermaid
graph LR
    subgraph æ•°æ®è´¨é‡è¦æ±‚
        D1[å¤šæ ·æ€§<br/>è¦†ç›–ä¸åŒä»»åŠ¡] --> Q[é«˜è´¨é‡æ•°æ®]
        D2[å‡†ç¡®æ€§<br/>ç­”æ¡ˆæ­£ç¡®è¯¦ç»†] --> Q
        D3[è§„èŒƒæ€§<br/>æ ¼å¼ä¸€è‡´æ— é”™] --> Q
    end
```

**é«˜è´¨é‡æ•°æ®çš„ç‰¹å¾**ï¼š
- ğŸ“Š **å¤šæ ·æ€§**ï¼šè¦†ç›–é—®ç­”ã€å†™ä½œã€ä»£ç ã€æ¨ç†ç­‰ä¸åŒä»»åŠ¡
- âœ… **å‡†ç¡®æ€§**ï¼šå›ç­”æ­£ç¡®ã€è¯¦ç»†ã€æœ‰å¸®åŠ©
- ğŸ“ **è§„èŒƒæ€§**ï¼šæ ¼å¼ç»Ÿä¸€ã€æ— è¯­æ³•é”™è¯¯

## 4. è®­ç»ƒæ–¹æ³•

### 4.1 å…¨å‚æ•°å¾®è°ƒ (Full Fine-tuning)

æ›´æ–°æ¨¡å‹æ‰€æœ‰å‚æ•°ï¼š

```mermaid
graph LR
    subgraph å…¨å‚æ•°å¾®è°ƒ
        I[è¾“å…¥] --> M[å…¨éƒ¨å‚æ•°<br/>å¯è®­ç»ƒ]
        M --> O[è¾“å‡º]
    end
```

```python
# å…¨å‚æ•°å¾®è°ƒ
for batch in dataloader:
    outputs = model(batch["input_ids"], labels=batch["labels"])
    loss = outputs.loss
    loss.backward()
    optimizer.step()
```

| ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|
| âœ… æ•ˆæœæœ€å¥½ | âŒ æ˜¾å­˜éœ€æ±‚å¤§ |
| âœ… å……åˆ†é€‚åº”æ–°ä»»åŠ¡ | âŒ å®¹æ˜“è¿‡æ‹Ÿåˆ |
| | âŒ æ¯ä¸ªä»»åŠ¡å­˜ä¸€ä»½æ¨¡å‹ |

### 4.2 LoRA (Low-Rank Adaptation)

**æ ¸å¿ƒæ€æƒ³**ï¼šå†»ç»“åŸå§‹å‚æ•°ï¼Œåªè®­ç»ƒä½ç§©åˆ†è§£çŸ©é˜µã€‚

```mermaid
graph LR
    subgraph LoRAåŸç†
        X[è¾“å…¥ x] --> W[åŸå§‹æƒé‡ W<br/>å†»ç»“]
        X --> L[LoRA: BA<br/>å¯è®­ç»ƒ]
        W --> ADD[+]
        L --> ADD
        ADD --> Y[è¾“å‡º]
    end
```

```
åŸå§‹æƒé‡: W (d Ã— d)
LoRA: W + Î”W = W + BÂ·A

å…¶ä¸­:
- B: (d Ã— r) çŸ©é˜µ
- A: (r Ã— d) çŸ©é˜µ  
- r << d (å¦‚ r=16, d=4096)

å¯è®­ç»ƒå‚æ•°: 2 Ã— d Ã— r << d Ã— d
```

```python
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=16,                    # ç§© (rank)
    lora_alpha=32,           # ç¼©æ”¾å› å­
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # åº”ç”¨ LoRA çš„æ¨¡å—
    lora_dropout=0.05,
)

model = get_peft_model(model, config)
model.print_trainable_parameters()
# trainable params: 4,194,304 || all params: 6,738,415,616 || trainable%: 0.0622%
```

**LoRA ä¼˜ç‚¹**ï¼š
- âœ… æ˜¾å­˜å ç”¨å°ï¼ˆ0.1% å‚æ•°ï¼‰
- âœ… è®­ç»ƒé€Ÿåº¦å¿«
- âœ… å¯åˆå¹¶å›åŸæ¨¡å‹
- âœ… å¯ä¸ºä¸åŒä»»åŠ¡è®­ç»ƒä¸åŒ adapter

### 4.3 QLoRA

LoRA + 4-bit é‡åŒ–ï¼Œè¿›ä¸€æ­¥é™ä½æ˜¾å­˜ï¼š

```mermaid
graph LR
    subgraph QLoRA
        M[æ¨¡å‹] --> Q[4-bité‡åŒ–<br/>NF4]
        Q --> L[LoRA<br/>BF16ç²¾åº¦]
    end
```

```python
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",           # NormalFloat4
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,      # åŒé‡é‡åŒ–
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
)
```

**æƒŠäººæ•ˆæœ**ï¼š
- ğŸ¯ å•å¼  **24GB GPU** å¯å¾®è°ƒ **65B** æ¨¡å‹ï¼
- ğŸ“‰ æ˜¾å­˜é™ä½ **4-8 å€**
- ğŸ”¥ æ•ˆæœæ¥è¿‘å…¨å‚æ•°å¾®è°ƒ

### 4.4 æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | å¯è®­ç»ƒå‚æ•° | 7B æ¨¡å‹æ˜¾å­˜ | æ•ˆæœ |
|------|-----------|-------------|------|
| **å…¨å‚æ•°** | 100% | ~60GB | æœ€å¥½ |
| **LoRA** | ~0.1% | ~16GB | å¾ˆå¥½ |
| **QLoRA** | ~0.1% | ~6GB | æ¥è¿‘LoRA |

## 5. è®­ç»ƒæŠ€å·§

### 5.1 åªè®¡ç®— Response çš„ Loss

```mermaid
graph LR
    subgraph Lossè®¡ç®—
        P[Prompt<br/>ä¸è®¡ç®—Loss] --> R[Response<br/>è®¡ç®—Loss]
    end
```

```python
# ä¸è®¡ç®— prompt éƒ¨åˆ†çš„ loss
labels = input_ids.clone()
labels[:, :prompt_length] = -100  # -100 è¡¨ç¤ºå¿½ç•¥
loss = model(input_ids, labels=labels).loss
```

### 5.2 å­¦ä¹ ç‡è®¾ç½®

SFT ä½¿ç”¨æ¯”é¢„è®­ç»ƒæ›´å°çš„å­¦ä¹ ç‡ï¼š

```python
# é¢„è®­ç»ƒ: lr = 1e-4 ~ 3e-4
# SFT:    lr = 1e-5 ~ 2e-5  (å°10å€)
```

### 5.3 Packing

å°†å¤šä¸ªçŸ­æ ·æœ¬æ‰“åŒ…æˆä¸€ä¸ªé•¿åºåˆ—ï¼Œæé«˜ GPU åˆ©ç”¨ç‡ï¼š

```
æ ·æœ¬1: [A1, A2, A3, PAD, PAD]
æ ·æœ¬2: [B1, B2, PAD, PAD, PAD]
æ ·æœ¬3: [C1, C2, C3, C4, PAD]

æ‰“åŒ…å: [A1, A2, A3, B1, B2, C1, C2, C3, C4]
        â†‘ æ— æµªè´¹çš„ paddingï¼
```

### 5.4 NEFTune

åœ¨ embedding å±‚æ·»åŠ å™ªå£°ï¼Œæå‡æ³›åŒ–èƒ½åŠ›ï¼š

```python
def noisy_embedding_forward(self, input_ids):
    embed = self.original_forward(input_ids)
    # æ·»åŠ å™ªå£°
    noise = torch.randn_like(embed) * self.noise_alpha / math.sqrt(embed.shape[-1])
    return embed + noise
```

**æ•ˆæœ**ï¼šåœ¨ AlpacaEval ä¸Šæå‡ 10%+ çš„èƒœç‡ï¼

## 6. å®æˆ˜ï¼šä½¿ç”¨ transformers + LoRA è®­ç»ƒ

```python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
)
from peft import LoraConfig, get_peft_model
from datasets import load_dataset

# 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.bfloat16,
)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
tokenizer.pad_token = tokenizer.eos_token

# 2. åº”ç”¨ LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
)
model = get_peft_model(model, lora_config)

# 3. åŠ è½½å’Œå¤„ç†æ•°æ®
dataset = load_dataset("tatsu-lab/alpaca")

def preprocess(examples):
    prompts = []
    for inst, inp, out in zip(examples["instruction"], examples["input"], examples["output"]):
        if inp:
            prompt = f"### Instruction:\n{inst}\n\n### Input:\n{inp}\n\n### Response:\n{out}"
        else:
            prompt = f"### Instruction:\n{inst}\n\n### Response:\n{out}"
        prompts.append(prompt)
    return tokenizer(prompts, truncation=True, max_length=512, padding="max_length")

dataset = dataset.map(preprocess, batched=True, remove_columns=dataset["train"].column_names)

# 4. è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir="./sft_lora_model",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    warmup_ratio=0.03,
    logging_steps=10,
    save_steps=500,
    bf16=True,
    report_to="wandb",
)

# 5. å¼€å§‹è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    data_collator=lambda data: {
        "input_ids": torch.stack([torch.tensor(d["input_ids"]) for d in data]),
        "attention_mask": torch.stack([torch.tensor(d["attention_mask"]) for d in data]),
        "labels": torch.stack([torch.tensor(d["input_ids"]) for d in data]),
    },
)
trainer.train()

# 6. ä¿å­˜ LoRA æƒé‡
model.save_pretrained("./sft_lora_model")
```

## 7. è¯„ä¼°æŒ‡æ ‡

### 7.1 è‡ªåŠ¨è¯„ä¼°

| æŒ‡æ ‡ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **Perplexity** | å›°æƒ‘åº¦ï¼Œè¶Šä½è¶Šå¥½ | è¯­è¨€å»ºæ¨¡è´¨é‡ |
| **BLEU/ROUGE** | ä¸å‚è€ƒç­”æ¡ˆçš„é‡åˆåº¦ | ç¿»è¯‘ã€æ‘˜è¦ |
| **Pass@k** | ä»£ç æµ‹è¯•é€šè¿‡ç‡ | ä»£ç ç”Ÿæˆ |
| **Exact Match** | ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ | é—®ç­” |

### 7.2 Benchmark è¯„æµ‹

```mermaid
graph TB
    subgraph å¸¸ç”¨Benchmark
        M[MMLU<br/>å¤šé¢†åŸŸçŸ¥è¯†]
        G[GSM8K<br/>æ•°å­¦æ¨ç†]
        H[HumanEval<br/>ä»£ç ç”Ÿæˆ]
        T[TruthfulQA<br/>çœŸå®æ€§]
        MT[MT-Bench<br/>å¤šè½®å¯¹è¯]
    end
```

| è¯„æµ‹é›† | è¯„ä¼°èƒ½åŠ› | æ ·æœ¬æ•° |
|--------|----------|--------|
| **MMLU** | 57 é¢†åŸŸçŸ¥è¯† | 14K |
| **GSM8K** | æ•°å­¦æ¨ç† | 8.5K |
| **HumanEval** | ä»£ç ç”Ÿæˆ | 164 |
| **TruthfulQA** | çœŸå®æ€§ | 817 |
| **MT-Bench** | å¤šè½®å¯¹è¯è´¨é‡ | 80 |

### 7.3 äººå·¥è¯„ä¼°

- **Helpfulness**ï¼šå›ç­”æ˜¯å¦æœ‰å¸®åŠ©
- **Harmlessness**ï¼šå›ç­”æ˜¯å¦å®‰å…¨æ— å®³
- **Honesty**ï¼šå›ç­”æ˜¯å¦è¯šå®

## 8. æœ¬ç« å°ç»“

```mermaid
mindmap
  root((SFT))
    ç›®æ ‡
      ç†è§£æŒ‡ä»¤
      ç”Ÿæˆæœ‰ç”¨å›ç­”
    æ•°æ®
      æ ¼å¼å¤šæ ·
      è´¨é‡ä¼˜å…ˆ
    æ–¹æ³•
      å…¨å‚æ•°å¾®è°ƒ
      LoRA
      QLoRA
    æŠ€å·§
      åªè®¡ç®—Response Loss
      Packing
      NEFTune
```

**æ ¸å¿ƒè¦ç‚¹**ï¼š
- âœ… SFT è®©é¢„è®­ç»ƒæ¨¡å‹å­¦ä¼šç†è§£å’Œéµå¾ªæŒ‡ä»¤
- âœ… æ•°æ®è´¨é‡æ¯”æ•°é‡æ›´é‡è¦ï¼ˆLIMA: 1000 æ¡å³å¯ï¼‰
- âœ… LoRA/QLoRA è®©ä¸ªäººä¹Ÿèƒ½å¾®è°ƒå¤§æ¨¡å‹
- âœ… åˆé€‚çš„è®­ç»ƒæŠ€å·§èƒ½æ˜¾è‘—æå‡æ•ˆæœ

## å»¶ä¼¸é˜…è¯»

- ğŸ“„ [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206)
- ğŸ“„ [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- ğŸ“„ [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- ğŸ“„ [NEFTune: Noisy Embeddings Improve Instruction Finetuning](https://arxiv.org/abs/2310.05914)

---

*ä¸‹ä¸€ç¯‡ï¼š[RL ç­–ç•¥ï¼šPPOã€GRPOã€DPO](./11.5-rl-strategies.md)*
