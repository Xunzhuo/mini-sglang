---
sidebar_position: 1
---

# 神经网络入门：从感知机到深度学习

在深入学习大语言模型之前，我们需要先理解神经网络的基本原理。本文将带你从最简单的感知机开始，逐步建立对深度学习的直觉。

## 从生物神经元到人工神经元

人工神经网络的灵感来源于人脑中的生物神经元。每个神经元接收来自其他神经元的信号，经过处理后决定是否"激活"并向下游传递信号。

### 感知机 (Perceptron)

感知机是最简单的神经网络单元，由 Frank Rosenblatt 于 1957 年提出：

```
输入: x₁, x₂, ..., xₙ
权重: w₁, w₂, ..., wₙ
偏置: b

输出: y = activation(w₁x₁ + w₂x₂ + ... + wₙxₙ + b)
```

### 激活函数 (Activation Function)

激活函数为神经网络引入**非线性**，使其能够学习复杂的模式：

| 函数 | 公式 | 特点 |
|------|------|------|
| **Sigmoid** | σ(x) = 1/(1+e⁻ˣ) | 输出 0-1，早期常用 |
| **Tanh** | tanh(x) | 输出 -1 到 1 |
| **ReLU** | max(0, x) | 简单高效，现代主流 |
| **GELU** | x·Φ(x) | Transformer 常用 |

## 多层神经网络

将多个神经元组织成层，再将多层连接起来，就构成了**多层感知机 (MLP)**：

```
输入层 → 隐藏层₁ → 隐藏层₂ → ... → 输出层
```

每一层的输出作为下一层的输入，这种结构被称为**前馈神经网络 (Feedforward Neural Network)**。

## 损失函数 (Loss Function)

损失函数衡量模型预测值与真实值之间的差距：

- **均方误差 (MSE)**：用于回归任务
  - `L = (1/n) Σ(y_pred - y_true)²`
  
- **交叉熵 (Cross-Entropy)**：用于分类任务
  - `L = -Σ y_true · log(y_pred)`

## 反向传播 (Backpropagation)

反向传播是训练神经网络的核心算法，基于**链式法则**计算损失函数对每个参数的梯度：

1. **前向传播**：输入数据，逐层计算，得到预测值
2. **计算损失**：比较预测值与真实值
3. **反向传播**：从输出层向输入层，逐层计算梯度
4. **参数更新**：使用梯度下降更新权重

```python
# 伪代码示意
for epoch in range(num_epochs):
    # 前向传播
    y_pred = model(x)
    
    # 计算损失
    loss = loss_function(y_pred, y_true)
    
    # 反向传播
    loss.backward()
    
    # 参数更新
    optimizer.step()
```

## 梯度下降与优化器

### 梯度下降 (Gradient Descent)

参数更新公式：`θ = θ - η · ∇L(θ)`

其中 η 是**学习率 (Learning Rate)**，决定每次更新的步长。

### 常用优化器

| 优化器 | 特点 |
|--------|------|
| **SGD** | 最基础，加入动量效果更好 |
| **Adam** | 自适应学习率，最常用 |
| **AdamW** | Adam + 权重衰减，LLM 训练首选 |

## 过拟合与正则化

### 过拟合 (Overfitting)

模型在训练数据上表现很好，但在新数据上表现差，说明模型"记住"了训练数据而非学到规律。

### 正则化技术

- **Dropout**：随机丢弃部分神经元
- **L2 正则化**：惩罚过大的权重
- **早停 (Early Stopping)**：验证集性能不再提升时停止训练

## 深度学习的崛起

为什么"深度"很重要？

1. **层次化特征提取**：底层学习简单特征，高层学习抽象概念
2. **表达能力**：更深的网络可以表示更复杂的函数
3. **参数效率**：相同参数量下，深层网络比宽层网络更强大

## 本章小结

- 神经网络由神经元、层、激活函数组成
- 通过反向传播和梯度下降进行训练
- 损失函数指导模型学习的方向
- 正则化技术防止过拟合

## 延伸阅读

- [3Blue1Brown: 神经网络系列视频](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- 《深度学习》(花书) 第 6 章：深度前馈网络

---

*下一篇：[语言模型简史](./02-language-model-history.md)*
